# -*- coding: utf-8 -*-
"""ML Methods.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q3u4dLeqBtEL-lFAz8M9KUoiVkH_Z9Me
"""

import numpy as np
import pandas as pd
import os

# Import for Scaling the features
from sklearn.preprocessing import StandardScaler

# Import for selecting the features that explain the most variance
from sklearn.decomposition import PCA

# Import for splitting the data into training and test sets
from sklearn.model_selection import train_test_split

# Import for performing cross-validation
from sklearn.model_selection import GridSearchCV

# Import for creating model pipelines
from sklearn.pipeline import Pipeline


from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier


import matplotlib.pyplot as plt
import seaborn as sns

from tensorflow.keras.utils import to_categorical
# import tensorflow as tf
# from keras.models import Sequential
# from keras.layers import Dense

eeg_df = pd.read_csv("/content/EEG.csv")

eeg_df.describe()

eeg_df = eeg_df.drop_duplicates()

eeg_df.shape

eeg_df.dtypes[eeg_df.dtypes == 'object']

Y1 = eeg_df['main.disorder']
Y2 = eeg_df['specific.disorder']
X = eeg_df.drop(['main.disorder', 'specific.disorder','eeg.date'], axis=1)

X.shape

X.dtypes[X.dtypes == 'object']

# Convert categorical variable to dummy/one-hot encoded variables
X1 = pd.get_dummies(X, columns=['sex'], drop_first=True)

# Replace all NaN values with 0
X1.fillna(0, inplace=True)

scaler = StandardScaler()
X1_rescaled = scaler.fit_transform(X1)

# Get the components that explain 95% of variance in the data
pca = PCA(n_components=0.95)
pca.fit(X1_rescaled)
reduced = pca.transform(X1_rescaled)
print("Number of components:", pca.n_components_)

print(reduced.shape)

# Split X and y into traint and test sets

X_train, X_test, y_train, y_test = train_test_split(reduced, Y1, test_size=0.2, random_state=42, stratify=eeg_df['main.disorder'])

# Print number of observations in X_train, X_test, y_train, and y_test
print(len(X_train), len(X_test), len(y_train), len(y_test))

# Example 1: Logistic Regression
log_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=2000)
log_reg.fit(X_train, y_train)
y_pred_log_reg = log_reg.predict(X_test)

# Example 2: Random Forest
rf_clf = RandomForestClassifier(n_estimators=100)
rf_clf.fit(X_train, y_train)
y_pred_rf = rf_clf.predict(X_test)

# Example 3: K-NN
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)
y_pred_knn = knn.predict(X_test)

# Example 4: XGBoost
xgb = XGBClassifier(use_label_encoder=True, eval_metric='mlogloss')  # use_label_encoder=False for compatibility with new versions
y = ['Addictive disorder', 'Anxiety disorder', 'Healthy control', 'Mood disorder',
     'Obsessive compulsive disorder', 'Schizophrenia', 'Trauma and stress related disorder']

# Encode string labels to integers
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

xgb.fit(X_train, y_train)
y_pred_xgb = xgb.predict(X_test)

# Evaluate the models
print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred_log_reg))
print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))
print("K-NN Accuracy:", accuracy_score(y_test, y_pred_knn))
print("XGBoost Accuracy:", accuracy_score(y_test, y_pred_xgb) )

# Print classification reports
print("\nClassification Report (Random Forest):")
print(classification_report(y_test, y_pred_rf))

xgb = XGBClassifier(use_label_encoder=True, eval_metric='mlogloss')  # use_label_encoder=False for compatibility with new versions
y = ['Addictive disorder', 'Anxiety disorder', 'Healthy control', 'Mood disorder',
     'Obsessive compulsive disorder', 'Schizophrenia', 'Trauma and stress related disorder']

# Encode string labels to integers
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Encode string labels to integers

X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)

# Initialize and train the XGBoost classifier
xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
xgb.fit(X_train, y_train)

# Predict on the test set
y_pred = xgb.predict(X_test)

# Decode the predictions back to original string labels if needed
y_pred_labels = label_encoder.inverse_transform(y_pred)
y_test_labels = label_encoder.inverse_transform(y_test)

# Evaluate the model
accuracy = accuracy_score(y_test_labels, y_pred_labels)
print("Accuracy:", accuracy)

# Print classification report and confusion matrix
print("\nClassification Report:\n", classification_report(y_test_labels, y_pred_labels))
print("\nConfusion Matrix:\n", confusion_matrix(y_test_labels, y_pred_labels))

print(eeg_df['main.disorder'].value_counts())

y_train.value_counts()

# Convert labels to categorical one-hot encoding
y_train_cat = to_categorical(y_train, num_classes=8)
y_test_cat = to_categorical(y_test, num_classes=8)

# Define a neural network model
model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')  # 10 classes with softmax for multiclass output
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train_cat, epochs=20, batch_size=32, validation_split=0.2)

# Evaluate on test data
test_loss, test_accuracy = model.evaluate(X_test, y_test_cat)
print("Neural Network Test Accuracy:", test_accuracy)
